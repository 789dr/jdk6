p1
import numpy as np
import matplotlib.pyplot as plt
def func(x,y):
return np.sin(x)**2+np.cos(y)**2
x=np.linspace(0,5,50)
y=np.linspace(0,5,50)
x,y=np.meshgrid(x,y)
z=func(x,y)
plt.contour(x,y,z,cmap='gist_rainbow_r');
plt.show()

p2
import numpy as np
import matplotlib.pyplot as plt
rad_arr=np.radians(np.linspace(0,360,20))
r_arr=np.arange(0,1,.1)
def func(r,theta):
return r*np.sin(theta)
r,theta=np.meshgrid(r_arr,rad_arr)
values=func(r,theta)
fig,ax=plt.subplots(subplot_kw=dict(projection='polar'))
ax.contourf(theta,r,values,cmp='Spectral_r')
plt.show()

import numpy as np
import plotly.figure_factory as ff
a=np.array([0.,0.,0.,0.,1./3,1./3,1./3,2./3,2./3,1.])
b=np.array([0.,1./3,2./3,1.,0.,1./3,2./3,0.,1./3,0.])
c=1-a-b
func=(a-0.02)*b*(a-0.5)*(b-0.4)*(c-1)**2
fig=ff.create_ternary_contour(np.array([a,b,c]),func,pole_labels=['a','b','c'],interp_mode='cartesian',colorscale='Viridis',)
fig.show()

p3
function fibonacci_search(f, a, b, n; E = 0.01):
s = (1-√5)/(1+√5)
p = 1/(1.618*(1-s^(n+1))/(1-s^n))
d = p*b + (1-p)*a
yd = f(d)
for i in 1 : n-1
print(a)
print("\n")
print(b)
print("\n")
if i == n-1
c = E*a + (1-E)*d
else
c = p*a + (1-p)*b
end
yc = f(c)
if yc < yd
b, d, yd = d, c, yc
else
a, b = b, c
end
p = 1/(1.618*(1-s^(n-i+1))/(1-s^(n-i)))end
return a < b ? (a, b) : (b, a)
end
function f(x):
return x*x-x+1
end
function f(x):
x^2
end
fibonacci_search(f,-5,15,7)

p4
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import clear_output
import time
def func_fx(x):
fx = np.sin(x)
return fx
def check_pos(x1,x2):
if(x1,x2):
label='right'
else:
label=''
return label
def update_interior(xl,xu):
d=((np.sqrt(5)-1)/2)*(xu-xl)print('d= ',d)
x1 = xl+d
x2 = xu-d
print('x1= ',x1,' x2= ',x2)
return x1,x2

def find_max(xl,xu,x1,x2,label):
fx1 = func_fx(x1)
fx2 = func_fx(x2)
if fx2>fx1 and label=='right':
xl = xl
xu = x1
new_x = update_interior(xl,xu)
x1 = new_x[0]
x2 = new_x[1]
xopt = x2
else:
xl = x2
xu = xu
new_x = update_interior(xl,xu)
x1 = new_x[0]
x2 = new_x[1]
xopt = x1
return xl,xu,xopt
def find_min(xl,xu,x1,x2,label):
fx1 = func_fx(x1)
fx2 = func_fx(x2)
if fx2>fx1 and label == 'right':
xl = x2
xu = xu
new_x = update_interior(xl,xu)
x1 = new_x[0]
x2 = new_x[1]
xopt = x1
else:
xl = xl
xu = x1
new_x = update_interior(xl,xu)
x1 = new_x[0]
x2 = new_x[1]
xopt = x1
print('xopt= ',xopt)
return xl,xu,xopt
def plot_graph(xl,xu,x1,x2):
clear_output(wait=True)
plt.plot(x,y)
plt.plot([0,6],[0,0],'k')
plt.plot(x1,func_fx(x1),'ro',label='x1')
plt.plot([x1,x1],[0,func_fx(x1)],'k')
plt.plot(x2,func_fx(x2),'bo',label='x2')
plt.plot([x2,x2],[0,func_fx(x2)],'k')
plt.plot([xl,xl],[0,func_fx(xl)])
plt.annotate('xl', xy=(xl-0.01,-0.2))
plt.plot([xu,xu],[0,func_fx(xu)])
plt.annotate('xu', xy=(xu-0.01,-0.2))
plt.plot([x1,x1],[0,func_fx(x1)],'k')
plt.annotate('x1', xy=(x1-0.01,-0.2))
plt.annotate('x2', xy=(x2-0.01,-0.2))
plt.ylim([-1.2,1.2])
plt.show()
def golden_search(xl,xu,mode,et):
it = 0
e=1
while e>et:
new_x = update_interior(xl,xu)
print('new_x= ',new_x)
x1 = new_x[0]
x2 = new_x[1]
print('x1= ',x1,' x2= ',x2)
fx1 = func_fx(x1)
fx2 = func_fx(x2)
label = check_pos(x1,x2)
clear_output(wait=True)
plot_graph(xl,xu,x1,x2) 
plt.show()
if mode=='max':new_boundary = find_max(xl,xu,x1,x2,label)
elif mode=='min':
new_boundary = find_min(xl,xu,x1,x2,label)
else:
print('please define min/max mode')
break
xl = new_boundary[0]
xu = new_boundary[1]
xopt = new_boundary[2]
it+=1
print('Iteration: ',it)
r = (np.sqrt(5)-1)/2
e = ((1-r )* (abs((xu-xl)/xopt)))*100 
print('Error= ',e)
time.sleep(1)
x = np.linspace(0,6,100)
y = func_fx(x)
golden_search(0,6,'max',0.05)

p5
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score
dataset = pd.read_csv('ball.csv')
sns.scatterplot(data=dataset, x='time', y='height', hue='time')
plt.title('time vs height of the ball')
plt.xlabel('time')
plt.ylabel('height')
plt.show()
model = np.poly1d(np.polyfit(dataset['time'],dataset['height'], 2))
polyline = np.linspace(0, 10, 100)
plt.scatter(dataset['time'], dataset['height'])
plt.plot(polyline, model(polyline))
plt.show()
print(model)
model(dataset['time'])))

p6
import numpy as np
import matplotlib.pyplot as plt
def mean_squared_error(y_true, y_predicted):
cost = np.sum((y_true - y_predicted)**2) / len(y_true)
return cost
def gradient_descent(x, y, iterations=1000, learning_rate=0.0001,
stopping_threshold=1e-6):
current_weight = 0.1
current_bias = 0.01
n = float(len(x))
costs = []
weights = []
previous_cost = None
for i in range(iterations):
y_predicted = (current_weight * x) + current_bias
current_cost = mean_squared_error(y, y_predicted)
if previous_cost and abs(previous_cost - current_cost) <= stopping_threshold:
break
previous_cost = current_cost
costs.append(current_cost)
weights.append(current_weight)
weight_derivative = -(2 / n) * sum(x * (y - y_predicted))
bias_derivative = -(2 / n) * sum(y - y_predicted)
current_weight = current_weight - (learning_rate * weight_derivative)
current_bias = current_bias - (learning_rate * bias_derivative)
if i % 1000 == 0:print(f"Iteration {i+1}: Cost {current_cost}, Weight {current_weight}, Bias
{current_bias}")
plt.figure(figsize=(8, 6))
plt.plot(weights, costs)
plt.scatter(weights, costs, marker='o', color='red')
plt.title("Cost vs Weights")
plt.ylabel("Cost")
plt.xlabel("Weight")
plt.show()
return current_weight, current_bias
def main():
X = np.array([32.50234527, 53.42680403, 61.53035803, 47.47563963, 59.81320787,
55.14218841, 52.21179669, 39.29956669, 48.10504169,52.55001444,
45.41973014, 54.35163488, 44.1640495 , 58.16847072,56.72720806,
48.95588857, 44.68719623, 60.29732685, 45.61864377,38.81681754])
Y = np.array([31.70700585, 68.77759598, 62.5623823 ,71.54663223, 87.23092513,
78.21151827, 79.64197305, 59.17148932, 75.3312423 , 71.30087989,
55.16567715, 82.47884676, 62.00892325, 75.39287043, 81.43619216,
60.72360244, 82.89250373, 97.37989686, 48.84715332, 56.87721319])
print(f"Estimated Weight: {estimated_weight}\nEstimated Bias: {estimated_bias}")
Y_pred = estimated_weight * X + estimated_bias
plt.figure(figsize=(8, 6))
plt.scatter(X, Y, marker='o', color='red')
plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='blue',
markerfacecolor='red',
markersize=10, linestyle='dashed')
plt.xlabel("X")
plt.ylabel("Y")
plt.show()
if __name__ == "__main__":
main()

p7
import numpy as np
import matplotlib.pyplot as plt
from numpy import arange, meshgrid
def objective(x1, x2):
return 5 * x1**2.0 + 7 * x2**2.0
def derivative_x1(x1, x2):
return 10.0 * x1
def derivative_x2(x1, x2):
return 14.0 * x2
x1 = arange(-5.0, 5.0, 0.1)x2 = arange(-5.0, 5.0, 0.1)
x1, x2 = meshgrid(x1, x2)
y = objective(x1, x2)
fig = plt.figure(figsize=(12, 4))
ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.plot_surface(x1, x2, y, cmap='viridis')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')
ax.set_title('3D plot of the objective function')
ax = fig.add_subplot(1, 2, 2)
ax.contour(x1, x2, y, cmap='viridis', levels=20)
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_title('Contour plot of the objective function')
def rmsprop(x1, x2, derivative_x1, derivative_x2, learning_rate, gamma, epsilon, max_epochs):
x1_trajectory = []
x2_trajectory = []
y_trajectory = []
x1_trajectory.append(x1)
x2_trajectory.append(x2)
y_trajectory.append(objective(x1, x2))
e1 = 0
e2 = 0
for _ in range(max_epochs):
gt_x1 = derivative_x1(x1, x2)
gt_x2 = derivative_x2(x1, x2)
e1 = gamma * e1 + (1 - gamma) * gt_x1**2.0
e2 = gamma * e2 + (1 - gamma) * gt_x2**2.0
x1 = x1 - learning_rate * gt_x1 / (np.sqrt(e1 + epsilon))
x2 = x2 - learning_rate * gt_x2 / (np.sqrt(e2 + epsilon))
x1_trajectory.append(x1)
x2_trajectory.append(x2)
y_trajectory.append(objective(x1, x2))
return x1_trajectory, x2_trajectory, y_trajectory
x1_initial = -4.0
x2_initial = 3.0
learning_rate = 0.1
gamma = 0.9
epsilon = 1e-8
max_epochs = 50
x1_trajectory, x2_trajectory, y_trajectory =
rmsprop(x1_initial,x2_initial,derivative_x1,derivative_x2,learning_rate,
gamma,epsilon,max_epochs)
print('The optimal value of x1 is:', x1_trajectory[-1])
print('The optimal value of x2 is:', x2_trajectory[-1])print('The optimal value of y is:', y_trajectory[-1])
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(1, 1, 1)
ax.contour(x1, x2, y, cmap='viridis', levels=20)
ax.plot(x1_trajectory, x2_trajectory, '*',
markersize=7, color='dodgerblue')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_title('RMSprop Optimization path for ' + str(max_epochs) + ' iterations')
plt.show()

p8
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
def distance(point1, point2):
return np.sqrt(np.sum((point1 - point2)**2))
def ant_colony_optimization(points, n_ants, n_iterations, alpha, beta,
evaporation_rate, Q):
n_points = len(points)
pheromone = np.ones((n_points, n_points))
shape & data type, where element value is set to 1
best_path = None
best_path_length = np.inf
for iteration in range(n_iterations):
paths = []
path_lengths = []
for ant in range(n_ants):
visited = [False]*n_pointscurrent_point = np.random.randint(n_points)
visited[current_point] = True
path = [current_point]
path_length = 0
while False in visited:
unvisited = np.where(np.logical_not(visited))[0]
probabilities = np.zeros(len(unvisited))
for i, unvisited_point in enumerate(unvisited):
probabilities[i] = pheromone[current_point, unvisited_point]**alpha /
distance(points[current_point], points[unvisited_point])**beta
probabilities /= np.sum(probabilities)
next_point = np.random.choice(unvisited, p=probabilities)
path.append(next_point)
path_length += distance(points[current_point], points[next_point])
visited[next_point] = True
current_point = next_point
paths.append(path)
path_lengths.append(path_length)
if path_length < best_path_length:best_path = path
best_path_length = path_length
pheromone *= evaporation_rate
for path, path_length in zip(paths, path_lengths):
for i in range(n_points-1):
pheromone[path[i], path[i+1]] += Q/path_length
pheromone[path[-1], path[0]] += Q/path_length
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(points[:,0], points[:,1], points[:,2], c='r', marker='o')
for i in range(n_points-1):
ax.plot([points[best_path[i],0], points[best_path[i+1],0]],
[points[best_path[i],1], points[best_path[i+1],1]],
[points[best_path[i],2], points[best_path[i+1],2]],
c='g', linestyle='-', linewidth=2, marker='o')
ax.plot([points[best_path[0],0], points[best_path[-1],0]],
[points[best_path[0],1], points[best_path[-1],1]],
[points[best_path[0],2], points[best_path[-1],2]],
c='g', linestyle='-', linewidth=2, marker='o')ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')
plt.show()
points = np.random.rand(10, 3) 
ant_colony_optimization(points, n_ants=10, n_iterations=100, alpha=1, beta=1,evaporation_rate=0.5, Q=1)
